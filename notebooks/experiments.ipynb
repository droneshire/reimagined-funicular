{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import dotenv\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_PLACES_API_KEY\")\n",
    "OPEN_AI_API_KEY = os.getenv(\"OPEN_AI_API_KEY\")\n",
    "\n",
    "CURRENT_DIR = %pwd\n",
    "ROOT_DIR = os.path.dirname(CURRENT_DIR)\n",
    "SRC_DIR = os.path.join(ROOT_DIR, \"src\")\n",
    "\n",
    "sys.path.append(SRC_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searches.google_places import GoogleMapsAPI, GooglePlacesAPI\n",
    "from searches.util import extract_city, get_city_center_coordinates\n",
    "from text_util import clean_text, parse_itenerary_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"location\": \"South Beach Miami, FL\",\n",
    "    \"number_of_people\": 6,\n",
    "    \"date\": \"November 2026\",\n",
    "    \"duration_days\": 3,\n",
    "    \"group_type\": \"bachelorette party\",\n",
    "    \"description\": (\n",
    "        \"include boutique hotel options must 4 stars higher onsite spa beach clubs djs\"\n",
    "        \"day high end nightclubs list least six restaurant options dinner include least \"\n",
    "        \"one nice steakhouse one nice sushi restaurant\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_START = \"\"\"Create itinerary: provide only names\n",
    "for places listed in google places for breakfast,\n",
    "morning activity, lunch, afternoon activity, dinner and\n",
    "evening activity. return only the day and the name:\n",
    "\"\"\".replace(\n",
    "    \"\\n\", \" \"\n",
    ")\n",
    "\n",
    "PROMPT_TEMPLATE = f\"\"\"“{inputs['duration_days']}” day\n",
    "“{inputs['group_type']}” to “{inputs['location']}” for “{inputs['number_of_people']}”\n",
    "people in “{inputs['date']}” that enjoy “{inputs['description']}”\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"{PROMPT_START}{clean_text(PROMPT_TEMPLATE)}\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimize Prompt Length**\n",
    "\n",
    "The cleaned_text contains a lot of detailed requirements for the itinerary. While detail is good, ensure that every word is necessary to fulfill the request. Extra tokens increase cost without adding value.\n",
    "\n",
    "**Adjust Temperature**\n",
    "\n",
    "A temperature of 1 is the most creative setting, leading to more varied outputs. However, for generating an itinerary based on specific criteria, a lower temperature might produce more consistent and focused results, potentially reducing the need for additional tokens to clarify or correct the output.\n",
    "\n",
    "**Refine Max Tokens**\n",
    "\n",
    "The max_tokens setting determines the maximum length of the generated response. If you find that the responses are consistently shorter than your limit, you can lower max_tokens to save on costs. Analyze the average length of the responses you're getting and adjust accordingly.\n",
    "\n",
    "**Use top_p Wisely**\n",
    "\n",
    "Setting top_p=1 means the model considers all possible next tokens at each step, which is fine for maximizing creativity but might not be necessary for your use case. Adjusting top_p to a lower value could make the model's responses more focused and concise, potentially reducing token usage.\n",
    "\n",
    "**Penalties**\n",
    "\n",
    "You've set frequency_penalty and presence_penalty to 0, which is neutral and doesn't influence the model's token choices. Depending on the variety you need in the responses, tweaking these values can help manage repetition and ensure the uniqueness of the places listed, potentially making the responses more efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPEN_AI_API_KEY)\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=256,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    ")\n",
    "print(response.choices[0].message.content.split(\"\\n\"))\n",
    "\n",
    "# Extract token usage from the response\n",
    "completion_tokens = response.usage.completion_tokens\n",
    "prompt_tokens = response.usage.prompt_tokens\n",
    "total_tokens = completion_tokens + prompt_tokens\n",
    "\n",
    "# Cost estimation for 1,000,000 requests\n",
    "total_requests = 1_000_000\n",
    "total_tokens_for_all_requests = total_tokens * total_requests\n",
    "\n",
    "training_cost_per_million_tokens = 8.00\n",
    "input_usage_cost_per_million_tokens = 3.00\n",
    "output_usage_cost_per_million_tokens = 6.00\n",
    "\n",
    "total_cost = (total_tokens_for_all_requests / 1_000_000) * (\n",
    "    training_cost_per_million_tokens\n",
    "    + input_usage_cost_per_million_tokens\n",
    "    + output_usage_cost_per_million_tokens\n",
    ")\n",
    "\n",
    "print(f\"Estimated total cost for {total_requests} requests: ${total_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2435,
     "status": "ok",
     "timestamp": 1711838165689,
     "user": {
      "displayName": "Aaron DeCoste",
      "userId": "09838251530013110066"
     },
     "user_tz": 240
    },
    "id": "N_qIxwmuFn4_",
    "outputId": "ad3575c2-4844-4ca5-d3a7-4e18d0202f51"
   },
   "outputs": [],
   "source": [
    "# Extract the itinerary content from the response\n",
    "itinerary_content = response.choices[0].message.content\n",
    "data = parse_itenerary_content(itinerary_content)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_places = GooglePlacesAPI(api_key=GOOGLE_API_KEY)\n",
    "google_maps = GoogleMapsAPI(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMA03IFWmqQKkWDwPiQuD4a",
   "provenance": [
    {
     "file_id": "1ew1f_XkMbNKy3rsJ9WuRr3vfsWskEU54",
     "timestamp": 1712353675429
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
