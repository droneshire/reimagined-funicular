{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import typing as T\n",
    "from threading import Lock\n",
    "\n",
    "import dotenv\n",
    "import googlemaps\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from openai import OpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_PLACES_API_KEY\")\n",
    "OPEN_AI_API_KEY = os.getenv(\"OPEN_AI_API_KEY\")\n",
    "\n",
    "CURRENT_DIR = %pwd\n",
    "ROOT_DIR = os.path.dirname(CURRENT_DIR)\n",
    "SRC_DIR = os.path.join(ROOT_DIR, \"src\")\n",
    "\n",
    "sys.path.append(SRC_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "gmaps = googlemaps.Client(key=GOOGLE_API_KEY)\n",
    "\n",
    "TYPES = [\n",
    "    \"bakery\",\n",
    "    \"sandwich_shop\",\n",
    "    \"coffee_shop\",\n",
    "    \"cafe\",\n",
    "    \"fast_food_restaurant\",\n",
    "    \"store\",\n",
    "    \"restaurant\",\n",
    "    \"food\",\n",
    "    \"point_of_interest\",\n",
    "    \"establishment\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METERS_PER_MILE = 1609.34\n",
    "\n",
    "\n",
    "def miles_to_meters(miles: float) -> float:\n",
    "    return miles * METERS_PER_MILE\n",
    "\n",
    "\n",
    "def get_city_center_coordinates(city_name: str) -> T.Optional[T.Tuple[float, float]]:\n",
    "    geolocator = Nominatim(user_agent=\"tgtg\")\n",
    "\n",
    "    location = geolocator.geocode(city_name)\n",
    "\n",
    "    if not location:\n",
    "        return None\n",
    "\n",
    "    return (location.latitude, location.longitude)\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Function to remove stop words, punctuation, and double quotes\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace('\"', \"\")\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Remove punctuation\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    cleaned_text = \" \".join(filtered_words)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def parse_itenerary_day(lines: T.List[str]) -> T.List[T.Tuple[str, str]]:\n",
    "    day_plan = []\n",
    "    # This pattern is designed to capture two groups separated by various delimiters\n",
    "    pattern = re.compile(r\"\\-?\\s*([\\w\\s]+?)\\s*(?::|at|-)\\s*([\\w\\s'&]+)\")\n",
    "\n",
    "    for line in lines:\n",
    "        match = pattern.match(line)\n",
    "        if match:\n",
    "            activity_type, place = match.groups()\n",
    "            activity_type = activity_type.strip()\n",
    "            place = place.split(\"(\")[0].strip()  # Removes anything within parentheses\n",
    "            day_plan.append((activity_type, place))\n",
    "        else:\n",
    "            print(f\"Could not parse line: {line}\")\n",
    "\n",
    "    return day_plan\n",
    "\n",
    "\n",
    "def parse_itenerary_content(content: str) -> T.List[T.Dict[str, str]]:\n",
    "    # Split the content into days and activities\n",
    "    days = content.split(\"\\n\\n\")\n",
    "    data = []\n",
    "    for day in days:\n",
    "        lines = day.split(\"\\n\")\n",
    "        day_number = lines[0].split(\" \")[1]\n",
    "        day_plan = parse_itenerary_day(lines[1:])\n",
    "        for activity_type, place in day_plan:\n",
    "            data.append({\"Day\": day_number, \"Activity Type\": activity_type, \"Place\": place})\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"location\": \"South Beach Miami, FL\",\n",
    "    \"number_of_people\": 6,\n",
    "    \"date\": \"November 2026\",\n",
    "    \"duration_days\": 3,\n",
    "    \"group_type\": \"bachelorette party\",\n",
    "    \"description\": (\n",
    "        \"include boutique hotel options must 4 stars higher onsite spa beach clubs djs\"\n",
    "        \"day high end nightclubs list least six restaurant options dinner include least \"\n",
    "        \"one nice steakhouse one nice sushi restaurant\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_START = \"\"\"Create itinerary: provide only names\n",
    "for places listed in google places for breakfast,\n",
    "morning activity, lunch, afternoon activity, dinner and\n",
    "evening activity. return only the day and the name:\n",
    "\"\"\".replace(\n",
    "    \"\\n\", \" \"\n",
    ")\n",
    "\n",
    "PROMPT_TEMPLATE = f\"\"\"“{inputs['duration_days']}” day\n",
    "“{inputs['group_type']}” to “{inputs['location']}” for “{inputs['number_of_people']}”\n",
    "people in “{inputs['date']}” that enjoy “{inputs['description']}”\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"{PROMPT_START}{clean_text(PROMPT_TEMPLATE)}\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPEN_AI_API_KEY)\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=256,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    ")\n",
    "print(response.choices[0].message.content.split(\"\\n\"))\n",
    "\n",
    "# Extract token usage from the response\n",
    "completion_tokens = response.usage.completion_tokens\n",
    "prompt_tokens = response.usage.prompt_tokens\n",
    "total_tokens = completion_tokens + prompt_tokens\n",
    "\n",
    "# Cost estimation for 1,000,000 requests\n",
    "total_requests = 1_000_000\n",
    "total_tokens_for_all_requests = total_tokens * total_requests\n",
    "\n",
    "training_cost_per_million_tokens = 8.00\n",
    "input_usage_cost_per_million_tokens = 3.00\n",
    "output_usage_cost_per_million_tokens = 6.00\n",
    "\n",
    "total_cost = (total_tokens_for_all_requests / 1_000_000) * (\n",
    "    training_cost_per_million_tokens\n",
    "    + input_usage_cost_per_million_tokens\n",
    "    + output_usage_cost_per_million_tokens\n",
    ")\n",
    "\n",
    "print(f\"Estimated total cost for {total_requests} requests: ${total_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the itinerary content from the response\n",
    "itinerary_content = response.choices[0].message.content\n",
    "data = parse_itenerary_content(itinerary_content)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for the nearby lookup, if store_type is left as None, it will\n",
    "# default to match the type of the place that we are searching nearby from\n",
    "keyword = \"breakfast\"\n",
    "store_type = \"restaurant\"\n",
    "radius_from_place_miles = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_coordinates = get_city_center_coordinates(inputs[\"location\"])\n",
    "print(f\"{inputs['location']} coordinates: {city_coordinates}\")\n",
    "\n",
    "assert store_type in TYPES, f\"Invalid store type: {store_type}, must be one of {','.join(TYPES)}\"\n",
    "\n",
    "itinerary_place_details = []\n",
    "nearby_place_details = {}\n",
    "\n",
    "lock = Lock()\n",
    "\n",
    "def get_place_details(row):\n",
    "    try:\n",
    "        result = gmaps.places(query=row[\"Place\"], location=city_coordinates)\n",
    "    except:\n",
    "        print(f\"Unable to get places info for {row['Place']}\")\n",
    "        return None\n",
    "\n",
    "    if not result or result.get(\"status\") != \"OK\":\n",
    "        print(f\"Unable to get places info for {row['Place']}\")\n",
    "        return None\n",
    "\n",
    "    place_result = result[\"results\"][0]\n",
    "    with lock:\n",
    "        itinerary_place_details.append(place_result)\n",
    "\n",
    "    radius_meters = miles_to_meters(radius_from_place_miles)\n",
    "\n",
    "    print(f\"Getting nearby places for {row['Place']} at {place_result['geometry']['location']}\")\n",
    "    try:\n",
    "        nearby_places = gmaps.places_nearby(\n",
    "            location=place_result[\"geometry\"][\"location\"],\n",
    "            radius=radius_meters,\n",
    "            keyword=keyword if keyword else None,\n",
    "            type=place_result.get(\"types\")[0] if place_result.get(\"types\") else TYPES,\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Unable to get nearby places info for {row['Place']}\")\n",
    "        return None\n",
    "\n",
    "    if not nearby_places or nearby_places.get(\"status\") != \"OK\":\n",
    "        print(f\"Unable to get nearby places info for {row['Place']}\")\n",
    "        return None\n",
    "\n",
    "    with lock:\n",
    "        nearby_place_details[row[\"Place\"]] = nearby_places[\"results\"]\n",
    "\n",
    "    return None\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(get_place_details, row) for _, row in df.iterrows()]\n",
    "    concurrent.futures.wait(futures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num results: {len(itinerary_place_details)}\")\n",
    "for item in itinerary_place_details:\n",
    "    print(item[\"name\"])\n",
    "    nearby_names = [i[\"name\"] for i in nearby_place_details.get(item[\"name\"], [])]\n",
    "    for name in nearby_names:\n",
    "        print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = time.time() - start\n",
    "\n",
    "print(f\"Total time taken: {total_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMA03IFWmqQKkWDwPiQuD4a",
   "provenance": [
    {
     "file_id": "1ew1f_XkMbNKy3rsJ9WuRr3vfsWskEU54",
     "timestamp": 1712353675429
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
